{
 "cells": [
  {
   "cell_type": "code",
   "id": "4ff106c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T11:37:42.861853Z",
     "start_time": "2025-05-08T11:37:42.567285Z"
    }
   },
   "source": [
    "import json\n",
    "import requests\n",
    "from gpt4all import GPT4All\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a69574fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:22:41.873388Z",
     "start_time": "2025-05-08T12:22:38.582504Z"
    }
   },
   "source": [
    "'''\n",
    "To use this code, you must download gtp4all and download the model \"qwen2 1_5b instruct\"\n",
    "'''\n",
    "\n",
    "#question = \"Act as a red agent in a cybersecurity context. Your goal is to disrupt a traffic simulation environment built with UXSIM. Propose and implement realistic cyber-attacks (such as DDoS, false demand injection, traffic signal manipulation, or data tampering) that can interfere with vehicle flow or deceive the traffic control logic. For each attack, provide a clear explanation of the objective, the attack method, and a working Python code snippet that integrates with the UXSIM framework.\"\n",
    "question = \"Can you give me 3 types of flowers and there more common diseases?\"\n",
    "\n",
    "# Send the request to ChatGPT\n",
    "model = GPT4All(\"nous-hermes-2-mistral-7b-dpo.Q4_0.gguf\")\n",
    "\n",
    "model.open()\n",
    "response=model.generate(question)\n",
    "\n",
    "# Print the result\n",
    "print(response)\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Request failed: HTTP 404 Not Found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m question \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan you give me 3 types of flowers and there more common diseases?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Send the request to ChatGPT\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m model \u001B[38;5;241m=\u001B[39m GPT4All(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnous-hermes-2-mistral-7b-dpo.Q4_0.gguf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mopen()\n\u001B[0;32m     12\u001B[0m response\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgenerate(question, max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\gpt4all\\gpt4all.py:235\u001B[0m, in \u001B[0;36mGPT4All.__init__\u001B[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001B[0m\n\u001B[0;32m    232\u001B[0m         device_init \u001B[38;5;241m=\u001B[39m _remove_prefix(device, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkompute:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    234\u001B[0m \u001B[38;5;66;03m# Retrieve model and download if allowed\u001B[39;00m\n\u001B[1;32m--> 235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig: ConfigType \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve_model(model_name, model_path\u001B[38;5;241m=\u001B[39mmodel_path, allow_download\u001B[38;5;241m=\u001B[39mallow_download, verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m LLModel(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m], n_ctx, ngl, backend)\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device_init \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\gpt4all\\gpt4all.py:341\u001B[0m, in \u001B[0;36mGPT4All.retrieve_model\u001B[1;34m(cls, model_name, model_path, allow_download, verbose)\u001B[0m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m allow_download:\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;66;03m# If model file does not exist, download\u001B[39;00m\n\u001B[0;32m    340\u001B[0m     filesize \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesize\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 341\u001B[0m     config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_model(\n\u001B[0;32m    342\u001B[0m         model_filename, model_path, verbose\u001B[38;5;241m=\u001B[39mverbose, url\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    343\u001B[0m         expected_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m filesize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mint\u001B[39m(filesize), expected_md5\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmd5sum\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    344\u001B[0m     ))\n\u001B[0;32m    345\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    346\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel file does not exist: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_dest\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\gpt4all\\gpt4all.py:393\u001B[0m, in \u001B[0;36mGPT4All.download_model\u001B[1;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001B[0m\n\u001B[0;32m    390\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected identity Content-Encoding, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[1;32m--> 393\u001B[0m response \u001B[38;5;241m=\u001B[39m make_request()\n\u001B[0;32m    395\u001B[0m total_size_in_bytes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent-length\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m    396\u001B[0m block_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m20\u001B[39m  \u001B[38;5;66;03m# 1 MB\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\gpt4all\\gpt4all.py:386\u001B[0m, in \u001B[0;36mGPT4All.download_model.<locals>.make_request\u001B[1;34m(offset)\u001B[0m\n\u001B[0;32m    384\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(url, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[0;32m    385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m206\u001B[39m):\n\u001B[1;32m--> 386\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRequest failed: HTTP \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mreason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m offset \u001B[38;5;129;01mand\u001B[39;00m (response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m206\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(offset) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent-Range\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConnection was interrupted and server does not support range requests\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Request failed: HTTP 404 Not Found"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ff371896ff55d2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
